{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This is a new version of a notebook that I originally worked up in a more experimental locale. It now operates using the [pymodelcat](https://github.com/usgs-biolab/pymodelcat) package that I started to house this work and uses a forked [sciencebasepy](https://github.com/skybristol/sciencebasepy/tree/weblink_structured_data_crawler) to handle web link annotation.\n",
    "\n",
    "Now that we have a [container](https://www.sciencebase.gov/catalog/item/5e8de96182cee42d134687cc) for model items in ScienceBase that we can operate against, it opens up all kinds of interesting possibilities for \"robots\" to do some work for us. This notebook explores what we might be able to get from the web links added into the mix. Theoretically, those represent a wealth of meaty material to help flesh out the model catalog as a useful resource. We can write some code that can go do some gathering, and then decide if there is anything that we can consistently bring back in to flesh out the items. Because we're doing this in code and this whole thing is experimental, we can be reasonably safe in writing information back into ScienceBase for use and evaluation. We just need to keep track of the parts of ScienceBase Items where we want to \"cede control to the robots\" and the parts we want to manage in some other way.\n",
    "\n",
    "There are several different strategies for leveraging links to gather more information. One interesting dynamic would be to simply index everything we find on subsequent landing pages and even spidering into the contents like any search engine. I've proposed in the past that ScienceBase could do this writ large, providing directed search engine functionality to go after linked content in meaningful ways.\n",
    "\n",
    "For this exercise, I'm focusing on a couple ways of fishing for structured metadata. This little robot is essentially the machine we talk about when we say things about \"machine-readable\" metadata or other content. A couple of potential strategies occur to me:\n",
    "\n",
    "* Content negotiation - Some web pages and applications accessible over HTTP enable content negotiation, which is a method for the requestor to negotiate the structure or substance of the content that is returned in a response. It can also do things like specify languages that content should be returned in. This is all based on what the server/application providing the content will actually support. You can't force the system to give you something it isn't prepared to deliver. Systems like ScienceBase and some other USGS platforms do support content negotiation with a couple different machine readable options, so it's worth trying to see what we might be able to use.\n",
    "* Structured metadata - There are a whole variety of ways that web systems have worked out to embed structured, machine-readable metadata within the HTML content delivered through web pages and web apps. Many of these coalesce around the schema.org set of content specifications to give us some degree of consistency to work from. One cool thing about these techniques is that they can be implemented right within the primary vehicle for web content delivery - the web page viewed by humans. Perhaps the coolest thing from a data science perspective is that the specifications and encoding methods really encourage explicit semantics (meaning we don't have to guess about what words mean), use of persistent identifiers and linking to associated registries (so we don't have to guess about disambiguating things), and linking between concepts that relate (so we can go after a wealth of information and tie it all together into a network/graph).\n",
    "\n",
    "Of these two, structured metadata probably offers interesting promise in terms of richness of content, consistency in resulting data, and overall ease of use. Unfortunately, the uptake of these methods in USGS is pretty abysmal, including in ScienceBase where we've not updated our use of schema.org metadata in 8 years or so.\n",
    "\n",
    "As a last resort, we could fall back on painful web scraping methods where we essentially parse HTML content and try to extract useful information. These are painful, because every one would essentially be a mostly custom affair that is probably more trouble than its worth. I do show a small bit of that here just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymodelcat.catbuilder import Catbuilder\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import qgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the functional logic to operate this robot is now housed in the Catbuilder class of the pymodelcat Python package. We instantiate the class as the cb object here and then operate the various functions. I'll explain what the functions do, but please reference the package code as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = Catbuilder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this codeblock, I use a get_models function to retrieve the model items in the model catalog. Right now, this returns a scaled down data structure that has just the stuff we care about working with most. We'll need to work up some options in this function for future use if we want to continue having a bit of a model-specific abstraction on the ScienceBase Item model via this API.\n",
    "\n",
    "> After migrating the link annotation code into my fork of sciencebasepy, I reworked the function to essentially work against each ScienceBase Item and its links as opposed to an earlier iteration where I pulled out all unique URLs from the entire collection and then ran those. After thinking through how this system would likely operate in production, working item by item seemed the best overall approach. We would ultimately want to establish some type of registry and associated API for every link check the system ever runs that would be decoupled from the links themselves. The registry would be checked by our code processes (likely operated as lambdas) and only run fresh periodically to check for new information nased on some business rules.\n",
    "\n",
    "At this point, I'm not concerned about what type of link we're dealing with. I can again come back and work on using the title of the web links (e.g., \"Model Reference Link\") to make decisions on what to do with what I find. In looking through the links, however, and the information that does come back on some of them, we might need to do a little more work to better classify just what these links mean when it comes to using content from their pages in a meaningful way (more on that later). To give a sense of what's returned here, I output a couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '5eb4485e82ce25b5135abf78',\n",
       "  'title': 'PHREEQC',\n",
       "  'webLinks': [{'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://www.usgs.gov/software/phreeqc-version-3',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Reference Link',\n",
       "    'hidden': False}]},\n",
       " {'id': '5eb4485f82ce25b5135abf84',\n",
       "  'title': 'Sagebrush Hurdle Model',\n",
       "  'webLinks': [{'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://code.usgs.gov/ecosystems/FRESC/sagebrush_hurdle_model',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Reference Link',\n",
       "    'hidden': False},\n",
       "   {'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://pubs.er.usgs.gov/publication/70204764',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Output Data',\n",
       "    'hidden': False},\n",
       "   {'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.14728',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Output Data',\n",
       "    'hidden': False}]},\n",
       " {'id': '5eb4486082ce25b5135abf94',\n",
       "  'title': 'SOILWAT2',\n",
       "  'webLinks': [{'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://github.com/DrylandEcology/SOILWAT2',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Reference Link',\n",
       "    'hidden': False}]},\n",
       " {'id': '5eb4485d82ce25b5135abf6a',\n",
       "  'title': 'MT3DMS',\n",
       "  'webLinks': [{'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://pubs.er.usgs.gov/publication/70189204',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Reference Link',\n",
       "    'hidden': False},\n",
       "   {'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://catalog.data.gov/dataset/modflow-2000-and-mt3dms-models-of-potentiometric-head-and-trichloroethene-concentration-at-the-',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Output Data',\n",
       "    'hidden': False}]},\n",
       " {'id': '5eb4486182ce25b5135abfa4',\n",
       "  'title': 'THRESH',\n",
       "  'webLinks': [{'type': 'webLink',\n",
       "    'typeLabel': 'Web Link',\n",
       "    'uri': 'https://www.usgs.gov/software/thresh',\n",
       "    'rel': 'related',\n",
       "    'title': 'Model Reference Link',\n",
       "    'hidden': False}]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = cb.get_models(fields=\"title,webLinks\")\n",
    "print(len(models))\n",
    "display(models[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our list of model items, we can now go out and figure out if we have anything interesting to work with. After working up the link examination process in the context of ScienceBase, I set it up to return an \"annotation\" data structure as a new key in each webLink for a given item. That gives us all the information we have to evaluate right inline with our other webLink information.\n",
    "\n",
    "The main sciencebasepy.Weblinks class has a set of functions that operate together in a configurable way on webLink records through the following logical components to gather potentially useful data.\n",
    "\n",
    "* Content negotiation is a whole topic in its own right that could use some careful consideration and more advanced methdos than I use here. As I said above, structured metadata holds a lot more interest and potential. In the current iteration, I check for either an application/xhtml+xml or application/json content structure. If valid XML is returned, I use a nice metadata parsing abstraction ([gis-metadata-parser](https://github.com/consbio/gis-metadata-parser)) put together by folks at the Conservation Biology Institute to grab up useful metadata properties from CSDGM, ISO, or ArcGIS metadata. If JSON is available, I just bring back the whole structure (this will need work).\n",
    "* For the structured metadata part, I've found the [extruct](https://github.com/scrapinghub/extruct) package from the ScrapingHub folks to be one of the most reliable, but it doesn't deal with quite all of the derivations folks have employed on embedding structured metadata in web pages. In this instance, I just try for anything we can get to evaluate for use.\n",
    "* The final thing threw in here as something of a Hail Mary if a basic web page meta scraper. It parses HTML content using the Python BeautifulSoup package and returns the page title and any named meta tags with content. This can sometimes yield a reasonable description depending on a lot of factors. This is a really terrible way to try and go about things for any kind of consisency given the vagaries of content management systems, legacy content, and all kinds of factors. But it's some other stuff to look at. I put this into a function that could be built upon further if it turns out this is actually a reasonable source to think about. There are also some more robust alternatives to this like the ScrapingHub AutoExtract API that we could think about.\n",
    "\n",
    "Running this kind of process in a big loop isn't a great method for some eventual production application. There are all kinds of considerations in doing this kind of thing in terms of having our robots be polite web crawlers, not freaking system administrators out with too many \"weird\" requests, and optimizing to check back in routinely over time for updated information. We could certainly paralellize this in a number of ways from launching Lambdas on the cloud to multithreading, and that would pull together data nice and quick. For now, working the relatively small number of links through in a loop is fine for demonstration and evaluation purposes.\n",
    "\n",
    "> Note: If you are fiddling with this and want to see what things look like before trying every URL, add something like ```[:5]``` to the end of the \"models\" list in the function to only send a limited number of items through the process.\n",
    "\n",
    "For our purposes at this point, we are really just trying to figure out what's useful out of this gathered information. I built a gathering process into Catbuilder that will run a list of model items and return either the ScienceBase Items with included annotation or flatten everything out and return a Pandas dataframe that might be useful for examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.usgs.gov/software/phreeqc-version-3']\n",
      "['https://code.usgs.gov/ecosystems/FRESC/sagebrush_hurdle_model', 'https://pubs.er.usgs.gov/publication/70204764', 'https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.14728']\n",
      "['https://github.com/DrylandEcology/SOILWAT2']\n",
      "['https://pubs.er.usgs.gov/publication/70189204', 'https://catalog.data.gov/dataset/modflow-2000-and-mt3dms-models-of-potentiometric-head-and-trichloroethene-concentration-at-the-']\n",
      "['https://www.usgs.gov/software/thresh']\n",
      "['https://coastalscience.noaa.gov/research/coastal-change/wemo/']\n",
      "['https://www.usgs.gov/software/multidimensional-surface-water-modeling-system-mdswms']\n",
      "['https://www.usgs.gov/mission-areas/water-resources/science/modflow-and-related-programs?qt-science_center_objects=0#qt-science_center_objects', 'https://www.usgs.gov/mission-areas/water-resources/science/modflow-and-related-programs?qt-science_center_objects=3#qt-science_center_objects', 'https://water.usgs.gov/lookup/getgislist', 'https://catalog.data.gov/dataset/modflow2000-and-modflow-asp-models-used-to-simulate-the-groundwater-flow-in-the-atlantic-c-2004']\n",
      "['https://www.usgs.gov/software/floodplain-and-channel-evaluation-tool-facet', 'https://www.usgs.gov/centers/sa-water/science/quantifying-floodplain-ecological-processes-and-ecosystem-services-delaware?qt-science_center_objects=0#qt-science_center_objects']\n",
      "['https://water.usgs.gov/ogw/bgas/frgt/']\n",
      "['https://www.usgs.gov/apps/MOViE/', 'https://doi.org/10.5066/P9VNZI1W']\n",
      "['https://www.usgs.gov/center-news/coupled-ocean-atmosphere-waves-sediment-transport-coawst-modeling-system-training?qt-news_science_products=2#qt-news_science_products']\n",
      "['https://www.usgs.gov/software/infil30']\n",
      "['https://water.usgs.gov/cgi-bin/man_wrdapp?daflow(1)']\n",
      "['https://water.usgs.gov/cgi-bin/man_wrdapp?modbrnch']\n",
      "['https://water.usgs.gov/software/MODFLOW-88/']\n",
      "['https://www.usgs.gov/software/water-energy-and-biogeochemical-model-webmod', 'https://pubs.er.usgs.gov/publication/tm6B35']\n",
      "['https://www.usgs.gov/software/vs2di-version-13', 'https://www.sciencebase.gov/catalog/item/5c7833aae4b0fe48cb4f5020', 'https://doi.org/10.3133/wri994130', 'https://doi.org/10.13031/2013.42238']\n",
      "['https://wwwbrr.cr.usgs.gov/projects/GWC_chemtherm/software.htm']\n",
      "['https://pubs.usgs.gov/of/2014/1073/']\n",
      "['https://www.usgs.gov/centers/wgsc/science/lucas-model?qt-science_center_objects=0#qt-science_center_objects']\n",
      "['https://www.usgs.gov/centers/new-england-water/science/seldm-stochastic-empirical-loading-and-dilution-model-project-page?qt-science_center_objects=0#qt-science_center_objects']\n",
      "['https://water.usgs.gov/software/SHARP/']\n",
      "['https://www.usgs.gov/centers/umid-water/science/development-a-fluegg-model-st-croix-river?qt-science_center_objects=0#qt-science_center_objects', 'https://doi.org/10.1016/j.jglr.2015.02.003']\n",
      "['https://www.usgs.gov/software/central-valley-hydrologic-model-cvhm']\n",
      "['https://pubs.usgs.gov/sir/2009/5026/section3.html']\n",
      "['https://www.usgs.gov/software/coupled-ground-water-and-surface-water-flow-model-gsflow']\n",
      "['https://www.usgs.gov/centers/pcmsc/science/coastal-storm-modeling-system-cosmos?qt-science_center_objects=0#qt-science_center_objects', 'https://www.sciencebase.gov/catalog/item/5633fea2e4b048076347f1cf', 'https://www.sciencebase.gov/catalog/item/57b12627e4b0fc09fab0ce4f']\n",
      "['https://pubs.er.usgs.gov/publication/ofr00107']\n",
      "['https://www.usgs.gov/software/seawat-a-computer-program-simulation-three-dimensional-variable-density-ground-water-flow', 'https://catalog.data.gov/dataset/gwm-2005-modflow-2005-modflow-nwt-and-seawat-2000-groundwater-flow-models-of-the-bedrock-aquife', 'https://doi.org/10.5066/P9URJ38Q', 'https://doi.org/10.5066/F7PV6HFR']\n",
      "['https://www.usgs.gov/software/slab2']\n",
      "['http://usgs-r.github.io/streamMetabolizer/', 'https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017JG004140']\n",
      "['https://water.usgs.gov/software/MODFE/']\n",
      "['https://water.usgs.gov/ogw/modflow-lgr/', 'https://pubs.usgs.gov/tm/6a44/', 'https://doi.org/10.3133/tm6A44']\n",
      "['https://pubs.er.usgs.gov/publication/ofr00107']\n",
      "['https://www.usgs.gov/software/ptcount']\n",
      "['https://www.usgs.gov/centers/wy-mt-water/science/probability-streamflow-permanence-prosper?qt-science_center_objects=0#qt-science_center_objects', 'https://www.usgs.gov/centers/wy-mt-water/science/probability-streamflow-permanence-prosper?qt-science_center_objects=4#qt-science_center_objects']\n",
      "['https://nc.water.usgs.gov/albe/General/SPARROW/sparrow.html', 'https://doi.org/10.5066/P9JSVCZX', 'https://doi.org/10.5066/P9NT387B']\n",
      "['https://www.sciencebase.gov/catalog/item/57c7ffbee4b0f2f0cebfc330']\n",
      "['https://github.com/usgs/nshmp/wiki/About-the-NSHMP']\n",
      "['https://water.usgs.gov/software/OTEQ/']\n",
      "['https://www.usgs.gov/land-resources/eros/lulc/land-cover-modeling-methodology-fore-sce-model?qt-science_support_page_related_con=4#qt-science_support_page_related_con', 'http://pubs.er.usgs.gov/publication/70194465']\n",
      "['https://wwwbrr.cr.usgs.gov/projects/GW_Solute/hst/']\n",
      "['https://catalog.data.gov/dataset/beware-database-a-bayesian-based-system-to-assess-wave-driven-flooding-hazards-on-coral-reef-li', 'https://doi.org/10.5066/F7T43S20']\n",
      "['https://water.usgs.gov/software/BLTM/']\n",
      "['https://water.usgs.gov/ogw/bgas/1dtemppro/', 'https://doi.org/10.5066/P9Q8JGAO']\n",
      "['https://water.usgs.gov/software/AnalyzeHOLE/', 'https://pubs.usgs.gov/tm/tm4f2/']\n",
      "['https://water.usgs.gov/software/FourPt/']\n",
      "['https://pubs.er.usgs.gov/publication/sir20195034', 'https://www.sciencebase.gov/catalog/item/527d0a83e4b0850ea0518326']\n",
      "['https://www.sciencebase.gov/catalog/item/50c30ff8e4b0b57f2415d172']\n",
      "['https://www.mbr-pwrc.usgs.gov/bbs/', 'https://doi.org/10.5066/F7JS9NHH']\n",
      "['https://water.usgs.gov/software/BIOMOC/']\n",
      "['https://water.usgs.gov/software/MODFLOW-96/']\n",
      "['https://www.usgs.gov/software/modpath-a-particle-tracking-model-modflow', 'https://catalog.data.gov/dataset/modflow-2000-and-modpath-models-for-simulations-used-to-delineate-contributing-areas-for-2017-p', 'https://catalog.data.gov/dataset/modflow-2000-and-modpath-used-to-evaluate-groundwater-flow-and-selected-groundwater-m-2004-2015', 'https://catalog.data.gov/dataset/modflow-2000-and-modpath-model-data-sets-used-in-scenarios-of-groundwater-flow-and-pumping-1900']\n",
      "['https://water.usgs.gov/nrp/gwsoftware/tdpf/tdpf.html']\n",
      "['https://www.usgs.gov/software/precipitation-runoff-modeling-system-prms', 'https://doi.org/10.3133/sir20135162']\n",
      "['https://github.com/usgs/nshmp-haz/wiki/ground-motion-models']\n",
      "['https://water.usgs.gov/cgi-bin/man_wrdapp?hspf', 'https://ntrl.ntis.gov/NTRL/dashboard/searchResults/titleDetail/PB97193114.xhtml']\n",
      "['https://www.usgs.gov/software/composite-raster-and-divergence-tool', 'https://doi.org/10.5066/F7ZC8239']\n",
      "['https://water.usgs.gov/cgi-bin/man_wrdapp?feswms-2dh']\n",
      "['https://www.usgs.gov/software/slammer']\n",
      "['https://water.usgs.gov/software/WSPRO/']\n",
      "['https://water.usgs.gov/software/BRANCH/']\n",
      "['https://www.usgs.gov/software/conduit-flow-process-cfp-a-program-simulate-turbulent-or-laminar-groundwater-flow']\n",
      "['https://www.usgs.gov/software/crt-cascade-routing-tool-define-and-visualize-flow-paths-grid-based-watershed-models']\n",
      "['https://water.usgs.gov/nrp/gwsoftware/mf2k_vsf/vsf.html']\n",
      "['https://doi.org/10.3133/sir20175006', 'https://www.sciencebase.gov/catalog/item/57c44124e4b0f2f0cebc8a2a']\n",
      "['https://volcanoes.usgs.gov/software/hydrotherm/']\n",
      "['https://water.usgs.gov/software/loadest/doc/']\n",
      "['https://www.usgs.gov/software/modflow-usg-unstructured-grid-version-modflow-simulating-groundwater-flow-and-tightly', 'https://doi.org/10.5066/F7C827DT', 'https://pubs.usgs.gov/tm/06/a45']\n",
      "['https://github.com/geoflows/D-Claw']\n",
      "['https://il.water.usgs.gov/proj/feq/']\n",
      "['https://catalog.data.gov/dataset/gflow-model-used-to-characterize-the-groundwater-resources-of-the-great-divide-unit-of-the-cheq']\n",
      "['https://www.usgs.gov/software/gwm-groundwater-management-process-modflow-using-optimization']\n",
      "['https://www.usgs.gov/software/exploration-and-graphics-river-trends-egret', 'https://dx.doi.org/10.3133/tm4A10']\n",
      "['https://water.usgs.gov/nrp/gwsoftware/tdpf/tdpf.html']\n",
      "['https://pubs.er.usgs.gov/publication/ofr00107']\n",
      "['https://ca.water.usgs.gov/projects/reg_hydro/basin-characterization-model.html']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://pubs.er.usgs.gov/publication/sir20095146']\n",
      "['https://www.usgs.gov/software/groundwater-transport-process-gwt']\n",
      "['https://www.usgs.gov/software/thornthwaite-monthly-water-balance-model', 'https://pubs.er.usgs.gov/publication/ofr20071088']\n",
      "['https://www.usgs.gov/software/trigrs']\n",
      "['https://doi.org/10.3133/ofr20181151', 'https://doi.org/10.5066/P9Q8JGAO']\n",
      "['https://doi.org/10.3133/tm6A41', 'https://www.usgs.gov/software/modpath-a-particle-tracking-model-modflow', 'https://doi.org/10.5066/F7TB151D', 'https://doi.org/10.5066/P9VWY11M']\n",
      "['https://www.usgs.gov/software/modflow-one-water-hydrologic-flow-model-conjunctive-use-simulation-software-mf-owhm']\n",
      "['https://www.usgs.gov/software/scoops3d']\n",
      "['https://ca.water.usgs.gov/modeling-software/basin-characterization-model.html']\n",
      "['https://www.usgs.gov/software/sutra-a-model-2d-or-3d-saturated-unsaturated-variable-density-ground-water-flow-solute-or', 'https://water.usgs.gov/cgi-bin/man_wrdapp?sutra(1)', 'https://pubs.er.usgs.gov/publication/tm6A52', 'https://dx.doi.org/10.5066/F7F47M8Q', 'https://doi.org/10.1088/1748-9326/aaf0cc', 'https://doi.org/10.5066/P9HWCOBP']\n",
      "['https://www.usgs.gov/software/swb-modified-thornthwaite-mather-soil-water-balance-code-estimating-groundwater-recharge', 'https://doi.org/10.3334/ORNLDAAC/1219', 'http://doi.org/10.5066/F7CJ8BMS', 'https://doi.org/10.5066/P9992YJM', 'https://doi.org/10.5066/F7XW4HRJ']\n",
      "['https://www.usgs.gov/software/massachusetts-reservoir-simulation-tool']\n",
      "['https://www.usgs.gov/software/pedestrian-evacuation-analyst-tool', 'https://www.sciencebase.gov/catalog/item/58ebcd55e4b0b4d95d320667?community=Hazards+Vulnerability+Analysis']\n",
      "['https://water.usgs.gov/ogw/pulse/']\n",
      "['https://www.umesc.usgs.gov/management/dss/sub_veg_model.html']\n",
      "['https://www.sciencebase.gov/catalog/item/58e7c2fee4b09da6799c0f17', 'https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.12679']\n",
      "['https://www.usgs.gov/software/modflow-nwt-a-newton-formulation-modflow-2005', 'https://doi.org/10.5066/F79P2ZRH', 'https://doi.org/10.5066/F7445JRM']\n",
      "['https://www.usgs.gov/software/iric-river-flow-and-riverbed-variation-analysis']\n",
      "['https://oss.deltares.nl/web/delft3d/about']\n",
      "['https://water.usgs.gov/nrp/gwsoftware/modflow2000/modflow2000.html', 'https://water.usgs.gov/nrp/gwsoftware/modflow2000/ofr00-92.pdf', 'https://pubs.er.usgs.gov/publication/ofr00466', 'https://doi.org/10.5066/P9DEOYGZ']\n",
      "['https://www.usgs.gov/software/modflow-2005-usgs-three-dimensional-finite-difference-ground-water-model', 'https://catalog.data.gov/dataset/gwm-2005-modflow-2005-modflow-nwt-and-seawat-2000-groundwater-flow-models-of-the-bedrock-aquife', 'https://doi.org/10.5066/P9E6INWZ', 'https://doi.org/10.5066/F770809V', 'https://pubs.er.usgs.gov/publication/sir20185136', 'https://doi.org/10.5066/F7Q52NXK']\n",
      "['https://www.usgs.gov/software/modflow-6-usgs-modular-hydrologic-model', 'https://doi.org/10.5066/P9K36P5S', 'https://doi.org/10.5066/P9O59RO0']\n",
      "['https://pubs.er.usgs.gov/publication/sir20195045']\n",
      "['https://pubs.usgs.gov/of/2012/1216/Presentations/Peter_Claggett.pdf']\n",
      "['https://water.usgs.gov/ogw/modflow-owhm/']\n",
      "['https://water.usgs.gov/software/OTIS/']\n",
      "['http://regclim.coas.oregonstate.edu/visualization/rcd/', 'http://regclim.coas.oregonstate.edu/dynamical-downscaling/', 'http://regclim.coas.oregonstate.edu/statistical-downscaling/northwest-US-and-southwest-Canada/']\n",
      "['https://pubs.usgs.gov/of/2015/1009/']\n",
      "['https://www.usgs.gov/software/winslamm']\n",
      "['https://water.usgs.gov/software/DR3M/']\n",
      "CPU times: user 23.2 s, sys: 792 ms, total: 24 s\n",
      "Wall time: 7min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "annotated_models = cb.annotate_model_links(models=models, output_format=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's the fun part: analyzing the data and seeing if there's anything we want to use. Well, it would be fun, except our results aren't actually all that consistent or robust.\n",
    "\n",
    "> We have very few cases where we were able to pull any useful structured metadata in at all, so our most promising route for a consistent and powerful method is stymied by the fact that most of the systems behind these links haven't implemented that method. As a little bit of a side tangent, someone really ought to be encouraging this type of thing across the USGS and perhaps leading by example. Beyond supporting what we're trying to do here, these methods would go a long way to improving how USGS content presents itself on the web, how we might influence search rankings, and how we might influence the various knowledge graph efforts toward recognizing USGS as an authority on some subjects.\n",
    "\n",
    "Content negotiation doesn't give us a whole lot of results either with a couple of notable exceptions. Somewhat by design, anything that points at a DOI link should respond to some type of accept header that will give us DOI metadata as a response.\n",
    "\n",
    "As clunky as it is, meta tag scraping may still be a viable option to try to work from (if we can avoid encouraging bad behavior). Meta tags are really not designed for modern robots as they do not really have capacity for explicit semantics to understand what the intent of the meta tags should be. It's all a matter of convention and usage within a particular context, and that has to be unraveled and dealt with in some fashion.\n",
    "\n",
    "Beyond the information structure, some of the research questions I would want to pursue include the following:\n",
    "\n",
    "* Can we legitimately use anything we bring back in these processes in a meaningful way to add more depth to our model catalog?\n",
    "* Can we use titles from any of these sources provide more than model acronyms or short names in our ScienceBase Items?\n",
    "* Do any of the descriptions make sense to serve as an abstract for the concept of the model as cataloged, or are they for some specific aspect of the modeling system?\n",
    "* Does our initial notional way of type classifying web links (e.g., Model Reference Link) help us in determining what information we can use?\n",
    "* How might the extended information from model related assets like model output data or software code be encoded into the model items to add value without adding confusion?\n",
    "\n",
    "To help facilitate looking through the results and set us up for the eventual capability we will want to build onto this, I set up a link_miner() function in Catbuilder. It is really crude at this point, and I know there are more efficient ways of working through the content. I focused on a couple of the more straightforward and content-rich types of structured schema.org content, XML metadata from content negotiation, and the higher level meta tags. The link_miner provides what is essentially a simple table containing a reference back to the model item in ScienceBase from which the links come from, the links and link types, and then the type of information, where it comes from in terms of an extraction method, and the content. Some of the content is pretty straightforward in terms of text strings, while for others, I left the basic raw content which could require further processing. For instance, there are lists of author names, which we could attempt to do some further work on.\n",
    "\n",
    "To facilitate looking at this information, I pull the list of dictionaries from the link_miner for every distinct piece of potentially useful information into a Pandas dataframe and then use the qgrid package to show the data in a filterable table. One thing you can do with this is to filder on \"link_classification\" to show Model Reference Links only and then filter info_type to select the most likely candidates for improved titles (title, name, and og:title are probably the ones to look at for this use). You can also dump the dataframe to CSV or Excel or some other format for use with other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_link_info = list()\n",
    "for model in annotated_models:\n",
    "    mined_link_info.extend(cb.link_miner(model, output_type=\"python\"))\n",
    "df_mined_link_info = pd.DataFrame(mined_link_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830c779ff3ce4d069fe79da04b17e521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defau…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qgrid.show_grid(df_mined_link_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mined_link_info.to_excel(\"AnnotatedModelInfo.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "There's lots more to do here, likely focusing in on the link_miner() function. This is something that we could also eventually abstract up to a higher level with sciencebasepy once we know enough about what we are looking for, where we can likely find the most useful information, and develop whatever tests are necessary to validate that gleaned information is indeed useful. It will be interesting to study just how much context matters in terms of the overall utility of the information in these different kinds of structured metadata content methods. My guess is that it will matter a good deal, and we may need different profiles for different kinds of content or sources of content to aid in determining the best use of the information contained. This is also where the schema.org methods should begin to become much more important (when implemented correctly) as they do help to force a certain degree of foresight into how information will be perceived when taken out of context by pushing for encoding of explicit context into the schema itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
